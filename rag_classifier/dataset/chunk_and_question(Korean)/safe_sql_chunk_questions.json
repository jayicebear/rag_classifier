[
  {
    "chunk_id": 0,
    "question": [
      "SAFE-SQL은 어떤 기술이나 방법론을 사용하여 텍스트를 SQL로 변환합니까?",
      "이 논문의 저자들은 어떤 기관에 소속되어 있습니까?",
      "SAFE-SQL의 주요 목적은 무엇인가요?"
    ],
    "content": "## SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL  \nJimin Lee 1 , Ingeol Baek 1 , Byeongjeong Kim 1 , Hyunkyung Bae 2 , Hwanhee Lee 1 * 1 Department of Artificial Intelligence, Chung-Ang University, 2 New York University {ljm1690, ingeolbaek, michael97k, hwanheelee}@cau.ac.kr , hb3135@nyu.edu"
  },
  {
    "chunk_id": 1,
    "question": [
      "Text-to-SQL의 목적은 무엇인가?",
      "SAFE-SQL의 주요 특징은 무엇이며, 어떻게 SQL 생성 품질을 향상시키는가?",
      "기존의 Text-to-SQL 접근 방식이 직면하는 한계는 무엇인가?"
    ],
    "content": "## Abstract  \nText-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose S elfA ugmentation incontext learning with F ine-grained E xample selection for Text-toSQL (SAFE-SQL), a novel unsupervised framework that enhances SQL generation by generating and intelligently filtering self-augmented examples. SAFE-SQL leverages an LLM to generate diverse Textto-SQL examples, which are then filtered by a novel fine-grained mechanism using criteria for semantic similarity, structural alignment, and reasoning path quality to curate highquality in-context learning examples. Leveraging these carefully selected self-generated examples, SAFE-SQL significantly surpasses previous zero-shot and few-shot Text-to-SQL frameworks, achieving superior execution accuracy. Notably, our approach demonstrates substantial performance gains in challenging extra hard and unseen scenarios, where conventional methods often struggle."
  },
  {
    "chunk_id": 2,
    "question": [
      "Text-to-SQL generation is primarily aimed at converting what into SQL queries?",
      "What challenges do traditional Text-to-SQL approaches face when translating user intent?",
      "What are the four key steps involved in the SAFE-SQL approach for enhancing SQL generation?"
    ],
    "content": "## 1 Introduction  \nText-to-SQL generation converts questions into SQL queries that help users access information in databases. Traditional approaches on Text-to-SQL rely on hand-crafted rules or simple pattern matching to generate SQL queries. They often struggle with the ambiguity and context-dependence of natural language, making it challenging to accurately translate user intent into structured SQL commands (El Boujddaini et al., 2024; Mohammadjafari et al., 2025; Li and Jagadish, 2014).  \n* Corresponding author.  \nFigure 1: The example on the left shows a failure in retrieving relevant examples due to masked keywords, which results in superficially similar but actually unrelated questions being selected. In contrast, our selfaugmented approach generates N-examples and filters them using 3 criteria, resulting in appropriate example retrieval.  \n<!-- image -->  \nAs the field progressed, more sophisticated approaches emerged, including skeleton-masked selection (Gao et al., 2023), relying on retrieving similar examples from training data to guide query generation. However, these methods face significant challenges in real-world scenarios where similar examples are often unavailable in the training set (Gan et al., 2021; Hong et al., 2024) or unrelated examples are retrieved as shown in Figure 1. To overcome these problems, recent research has introduced methods to generate synthetic data. SQL-GEN, presents by (Pourreza et al., 2024), introduces dialect-specific synthetic data to resolve the diverse SQL dialect challenges in Text-to-SQL systems. Another important aspect of synthetic data generation is incorporating key relationships from the schema and employing schema-distance-weighted column sampling (Zhao et al., 2022). However, these synthetic data generation methodologies predominantly require supervised fine-tuning, which demands substantial computational resources and time (Yang et al.,  \n2024b). Moreover, self-generated examples can introduce significant noise and inaccuracies that undermine the quality of in-context learning. Errors in synthetic SQL queries or flawed reasoning paths may lead to incorrect interpretations of database schemas (Wretblad et al., 2024). As a result, relying on unfiltered self-generated examples for Text-to-SQL tasks can pose a risk of degrading overall model performance. Consequently, it is necessary to develop more efficient approaches that enhance the accuracy of Text-to-SQL while eliminating extra training costs and mitigating the adverse impacts of noisy self-generated examples by implementing a robust filtering mechanism.  \nIn this paper, we propose SAFE-SQL, a novel approach that fully exploits the generative power of large language models (LLMs) to create highquality synthetic examples in an unsupervised manner. SAFE-SQL enhances its inference capabilities without additional fine-tuning through four key steps: (1) Schema Linking : Analyzing SQL test questions, database tables, and foreign keys to map relationships between queries and database structures (2) Example Generation : Generating N-question-SQL query-reasoning path triplets per input using schema-linked information with LLMs (3) Threshold-based example selection : Filtering generated examples using specifically designed relevance criteria based on semantic similarity, Structural alignment, and reasoning path validity, retaining only those scoring above a specific threshold to ensure high quality and relevance for in-context learning examples and (4) Final SQL Inference : Leveraging the curated examples, this step utilizes in-context learning to enhance the performance of large language models. This approach benefits from carefully selected examples that align with the natural language question and database schema, ensuring accurate and efficient SQL generation.  \nBy relying on LLM-generated and filtered examples, SAFE-SQL significantly improves robustness and accuracy, particularly in complex or unseen scenarios where retrieval-based approaches struggle. Our approach eliminates the need for additional model training while achieving superior performance in Text-to-SQL tasks. Our contributions can be listed as follows:  \n- We propose SAFE-SQL, a fully unsupervised approach that leverages LLMs to generate synthetic examples.\n- Our method leverages schema linking to dynam-  \nically adapt examples, boosting the performance of Text-to-SQL in complex scenarios.  \n- We introduce a structured filtering mechanism that selects high-quality question-SQL pairs based on semantic similarity, structural alignment, and reasoning path validation."
  },
  {
    "chunk_id": 3,
    "question": [
      "RAT-SQL과 PICARD의 주요 차이점은 무엇인가요?",
      "자연어 질문과 데이터베이스 스키마의 구조 정보가 SQL 생성에 미치는 영향은 무엇인가요?",
      "SELF-Instruct의 목적은 무엇이며, 어떤 방법을 통해 instruction-tuning 데이터를 생성하나요?"
    ],
    "content": "## 2 Related Work  \nStructural and Semantic Information for Textto-SQL Advances in Text-to-SQL have increasingly emphasized the importance of effectively utilizing structural and semantic information derived from the database schema. RAT-SQL (Wang et al., 2021) introduces relation-aware transformer architectures capable of encoding both the natural language question and the complex structure of the database schema, leading to improved schema linking performance. Concurrently, PICARD (Scholak et al., 2021) demonstrates that leveraging constrained decoding with step-by-step execution during generation can reduce the likelihood of producing invalid SQL queries. Rather than treating SQL generation as a pure sequence prediction task, PICARD executes partial SQL statements during generation, thus enforcing syntactic and semantic correctness. Both of these approaches highlight the importance of integrating structural and semantic information to generate correct SQL queries. Building on this line of research, our work explores how self-augmented examples can effectively incorporate structurally and semantically relevant information for in-context learning, particularly in scenarios where large annotated datasets are unavailable.  \nIn-context Learning with Example Augmentation and Filtering As LLMs have demonstrated strong performance in in-context learning settings, recent work has focused on improving the effectiveness of demonstrations through better example augmentation and selection (Toteja et al., 2025). Self-Instruct (Wang et al., 2023) introduces a framework for generating instructiontuning data by prompting the model to synthesize and filter examples, leveraging synthetic supervision. Further studies on demonstration selection for in-context learning (Wang et al., 2024) have systematically studied strategies for selecting effective in-context examples, including similaritybased retrieval and clustering methods. Integrating self-generated prompts with explicit reasoning  \nFigure 2: Overall flow of our proposed SAFE-SQL.  \n<!-- image -->  \nchains has also been shown to significantly improve in-context learning outcomes by guiding the model's thought process (Shum et al., 2023; Wei et al., 2023). These studies collectively underscore the impact of demonstration quality on LLM performance and highlight the potential of intelligently curating examples. Focusing on the Text-to-SQL task, our work distinguishes itself by generating synthetic examples and filtering them using a novel fine-grained mechanism that considers semantic similarity, structural similarity, and reasoning path quality. This allows us to generate multiple candidate examples and select the most effective through this tailored fine-grained filtering process."
  },
  {
    "chunk_id": 4,
    "question": [
      "SAFE-SQL은 무엇을 위해 설계되었나요?",
      "SAFE-SQL이 기존 방법들과 다른 점은 무엇인가요?",
      "SAFE-SQL에서 생성된 예제는 어떤 기준으로 필터링되나요?"
    ],
    "content": "## 3 Fine-grained Self-Augmentation for Text-to-SQL  \nWe propose SAFE-SQL, a framework that automatically generates high-quality examples for incontext learning in Text-to-SQL tasks. Unlike traditional methods that rely on retrieving similar questions or using predefined templates, SAFE-SQL uses LLMs to create synthetic examples tailored to the given database schema. These examples are then filtered based on their semantic similarity , structural alignment , and the quality of reasoning paths . Finally, we predict the final SQL query for the test input using the self-generated examples via in-context learning."
  },
  {
    "chunk_id": 5,
    "question": [
      "SAFE-SQL의 첫 번째 단계는 무엇이며, 그 목적은 무엇인가?",
      "스키마 링크 단계에서 어떤 요소들이 분석되는가?",
      "스키마 링크가 Text-to-SQL 작업의 성능 향상에 기여하는 방법은 무엇인가?"
    ],
    "content": "## 3.1 Schema Linking  \nThe first step in SAFE-SQL is schema linking, which identifies and extracts relevant schema elements from the database to reduce noise and improve performance in Text-to-SQL tasks (Cao et al., 2024). As shown in Figure 2, the schema linking step involves analyzing the test question to detect keywords and phrases that correspond to schema elements such as tables, columns, rows, and foreign keys within the database schema. This mapping narrows the focus to the most pertinent parts of the schema and provides the necessary context for generating relevant examples that are both meaningful and grounded in the database structure."
  },
  {
    "chunk_id": 6,
    "question": [
      "LLM은 각 테스트 질문에 대해 몇 개의 합성 예시를 생성하나요?",
      "생성된 SQL 질문들은 어떤 요소들이 다양화되면서도 구조적 유사성을 유지하나요?",
      "이유 경로는 무엇을 설명하며, 그것이 SQL 쿼리 실행 과정에서 어떤 역할을 하나요?"
    ],
    "content": "## 3.2 Example Generation  \nUsing the information obtained from schema linking, the LLM generates a pool of multiple synthetic examples for each test question. As illustrated in Figure 2, for each test question, we generate ten examples-each comprising a similar question, its corresponding SQL query, and a detailed reasoning path. The generated SQL questions maintain structural similarity while varying elements such as numerical values, table names, and key attributes. This ensures that the generated examples remain relevant while encouraging the model to generalize beyond surface-level patterns. By observing these modified instances, the model can infer the correct SQL query even when face with unseen but structurally similar questions. In particular, the reasoning path outlines the logical steps required to derive the correct SQL query result, providing a comprehensive explanation of the query execution process. We provide the full prompt used for LLMs in Appendix B.1."
  },
  {
    "chunk_id": 7,
    "question": [
      "SAFESQL의 평가 과정에서 생성된 예제의 관련성을 어떻게 결정하나요?",
      "세 가지 구성 요소인 의미 유사성, 구조적 정렬, 추론 경로 품질은 각각 무엇을 평가하나요?",
      "SAFESQL에서 LLM은 각 구성 요소의 점수를 어떻게 계산하나요?"
    ],
    "content": "## 3.3 Relevance Scoring  \nAfter generating a set of synthetic examples, SAFESQL employs a crucial evaluation process rooted in novel fine-grained example selection to determine the relevance of each generated example to the test question. This fine-grained selection process is integral to our method, ensuring that only high-quality, contextually appropriate examples are used for in-context learning, moving beyond simple retrieval to curate examples truly relevant and beneficial for the Text-to-SQL task.  \nTo achieve this, we assign composite relevance score Rel on a scale from 0 to 10 to each example e , which is calculated as follows:  \n<!-- formula-not-decoded -->  \nHere, Q t represents the test question, Q e denotes the generated example question. The coefficients α , β , and γ are weighting factors that sum to 1, allowing for adjustment of the relative importance of each component in the fine-grained selection score. The three components are defined as follows:  \n- Semantic Similarity S ( Q e , Q t ) : assesses if the generated question preserves the underlying meaning and intent to ensure the example aligns with the user's core query objective.\n- Structural Alignment A ( Q e , Q t ) : evaluates structural correspondence based on key database elements and their relationships, which is important for mapping natural language to a similar database structure and operations.\n- Reasoning Path Quality R : evaluates the alignment of the example's logical derivation steps and database operations (e.g., filtering, aggregation, joins, subqueries) with the test question's required logic.  \nWe utilize LLMs to compute the score for each of the three components based on our specifically designed instructions and criteria. Specifically, the LLM applies a predefined, multi-point scoring rubric (detailed in Appendix B.2) to assign a quantitative score (0-10) for each criterion. This process allows for a nuanced assessment of the degree of alignment between the generated example and the test question along each dimension, moving beyond simple binary or qualitative judgments. By carefully evaluating these three factors through our fine-grained example selection process, SAFESQL ensures that the selected examples are highly relevant and informative, contributing to more accurate and effective SQL query generation."
  },
  {
    "chunk_id": 8,
    "question": [
      "SAFE-SQL은 어떤 기준에 따라 예제를 선택하는가?",
      "임계값 θ는 무엇을 의미하며, 왜 설정되었는가?",
      "임계값 설정이 SQL 생성에 어떤 영향을 미치는가?"
    ],
    "content": "## 3.4 Threshold Selection  \nTo further ensure quality, SAFE-SQL retains only those examples with a relevance score above a predefined threshold θ . Formally, the set of selected examples is defined as:  \n<!-- formula-not-decoded -->  \nwhere E represents all generated examples. This thresholding step filters out low-quality examples and ensures that only the most informative and contextually appropriate examples are used in the final inference. The threshold is set to 8, as Figure 4 demonstrates that this value provides an optimal balance between preserving high-quality examples and maintaining sufficient diversity for robust SQL generation."
  },
  {
    "chunk_id": 9,
    "question": [
      "최종 단계에서 생성된 고품질 예제는 무엇과 결합되어 포괄적인 프롬프트를 구성하는가?",
      "SAFE-SQL은 어떤 방식으로 SQL 쿼리를 생성하는가?",
      "이 문단에서 언급된 '해석 가능한 추론 과정'은 무엇을 의미하는가?"
    ],
    "content": "## 3.5 Final Inference  \nIn the final stage, the high-quality examples generated in previous steps are combined with the test question to construct a comprehensive prompt for the LLM. These examples, enriched with filtered questions, corresponding SQL queries, and detailed reasoning paths, guide the LLM in generating the final SQL query. By integrating schema linking, synthetic example generation, relevance scoring, and threshold-based filtering, SAFE-SQL produces SQL queries that are both syntactically correct and semantically aligned with the intended database operations, while also providing an interpretable reasoning process."
  },
  {
    "chunk_id": 10,
    "question": [
      "실험의 주제는 무엇인가요?",
      "이 실험에서 어떤 방법론이 사용되었나요?",
      "실험의 결과는 무엇을 시사하나요?"
    ],
    "content": "## 4 Experiment"
  },
  {
    "chunk_id": 11,
    "question": [
      "실험에서 비교되는 모델들은 무엇인가요?",
      "Spider dev 데이터셋의 구성은 어떻게 되나요?",
      "BIRD 데이터셋은 어떤 종류의 데이터를 포함하고 있나요?"
    ],
    "content": "## 4.1 Experimental Setup  \nFor our experiments, we employ six models for comparison purposes: GPT-4o (Hurst et al., 2024), GPT-4o-mini (Hurst et al., 2024), GPT-4 (Achiam et al., 2023), Llama-3.1-70B-Instruct (Dubey et al., 2024), Llama3.3-70B-Instruct (Dubey et al., 2024),  \nQwen2.5-72B (Yang et al., 2024a), Gemma312b (Team et al., 2025), and Gemma3-27b (Team et al., 2025). The evaluation is conducted on the Spider dev dataset (Yu et al., 2018) and the Bird dev dataset (Li et al., 2023), which are widely used benchmarks for Text-to-SQL systems. The Spider dev set contains 7,000 training samples covering 166 databases in various domains and 1,034 evaluation samples from 20 databases, comprised of four difficulty levels. BIRD is a large cross-domain Text-to-SQL dataset with 12,751 question-SQL pairs across 95 databases. Since the test sets of both the Spider and BIRD datasets are only accessible through specific evaluation servers, we conduct our evaluation using their respective development sets."
  },
  {
    "chunk_id": 12,
    "question": [
      "What is the purpose of supervised fine tuning in Text-to-SQL methods?",
      "How does SQLPaLM utilize synthetic data augmentation?",
      "What approach does Din SQL take to enhance the reasoning process of large language models?"
    ],
    "content": "## 4.2 Baselines  \nWe use the following baseline Text-to-SQL methods: Supervised fine tuning , which fine-tunes an open source model, Zero-shot , which infers without examples, Few-shot , which infers with few examples. Synthesizing Text-to-SQL data from weak and strong LLMs (Yang et al., 2024b) utilizes preference learning from the weak data from small LLMs and strong data from LLMs. SQLPaLM (Sun et al., 2024) introduces synthetic data augmentation to fine-tune open source models. Din SQL (Pourreza and Rafiei, 2023) breaking down the task into smaller sub-tasks, allowing large language models to improve their reasoning process through self-correction iteratively. C3-SQL (Dong et al., 2023) comprises clear prompting , calibration with hints , and consistent output , which systematically addresses model input, bias, and output to enhance performance using the zero-shot prompt. Dail-SQL (Gao et al., 2023) introduces effective few-shot learning, significantly reducing the number of tokens required per question. ACTSQL (Zhang et al., 2023) enhances Text-to-SQL performance by automatically generating chain-ofthought exemplars, eliminating the need for manual labeling. PTD-SQL (Luo et al., 2024) categorizes queries into subproblems and focuses on targeted drilling to improve LLMs' reasoning capabilities."
  },
  {
    "chunk_id": 13,
    "question": [
      "What are the two metrics used to evaluate the model's performance?",
      "How is Execution Accuracy (EX) defined in the context of SQL query evaluation?",
      "What does Exact Match (EM) assess regarding the predicted SQL queries?"
    ],
    "content": "## 4.3 Evaluation Metrics  \nWe use Execution Accuracy (EX) and Exact Match (EM) to evaluate the performance of our model. EX measures whether the SQL query generated by the model produces the same results as the ground truth query when executed on a database. Exact Match  \nTable 1: Execution accuracy across difficulty levels on the Spider development set. The highest score per row is in bold, and the second highest is underlined.  \n| Method                             | Model                              | Easy                               | Medium                             | Hard                               | Extra                              | All                                | Time                               |\n|------------------------------------|------------------------------------|------------------------------------|------------------------------------|------------------------------------|------------------------------------|------------------------------------|------------------------------------|\n| Supervised Fine-Tuning (SFT)       | Supervised Fine-Tuning (SFT)       | Supervised Fine-Tuning (SFT)       | Supervised Fine-Tuning (SFT)       | Supervised Fine-Tuning (SFT)       | Supervised Fine-Tuning (SFT)       | Supervised Fine-Tuning (SFT)       | Supervised Fine-Tuning (SFT)       |\n| SYN-SQL                            | Sense-13B                          | 95.2                               | 88.6                               | 75.9                               | 60.3                               | 83.5                               | -                                  |\n| SQL-PaLM                           | PaLM-2                             | 93.5                               | 84.8                               | 62.6                               | 48.2                               | 77.3                               | -                                  |\n| Zero-shot Methods                  | Zero-shot Methods                  | Zero-shot Methods                  | Zero-shot Methods                  | Zero-shot Methods                  | Zero-shot Methods                  | Zero-shot Methods                  | Zero-shot Methods                  |\n| Baseline                           | GPT-4                              | 84.3                               | 73.1                               | 65.8                               | 40.3                               | 69.1                               | 1.28                               |\n| Baseline                           | GPT-4o                             | 87.2                               | 77.2                               | 68.4                               | 48.7                               | 73.4                               | 0.93                               |\n| Baseline                           | GPT-4o-mini                        | 84.8                               | 75.6                               | 67.0                               | 46.1                               | 71.5                               | 1.07                               |\n| C3-SQL                             | GPT-4                              | 90.2                               | 82.8                               | 77.3                               | 64.3                               | 80.6                               | 19.34                              |\n| Few-shot Methods                   | Few-shot Methods                   | Few-shot Methods                   | Few-shot Methods                   | Few-shot Methods                   | Few-shot Methods                   | Few-shot Methods                   | Few-shot Methods                   |\n| DIN-SQL                            | GPT-4                              | 91.1                               | 79.8                               | 64.9                               | 43.4                               | 74.2                               | 4.37                               |\n| DAIL-SQL                           | GPT-4                              | 91.9                               | 90.1                               | 75.2                               | 63.8                               | 83.6                               | 16.79                              |\n| ACT-SQL                            | GPT-4                              | 91.1                               | 79.4                               | 67.2                               | 44.0                               | 74.5                               | 4.55                               |\n| PTD-SQL                            | GPT-4                              | 94.8                               | 88.8                               | 85.1                               | 64.5                               | 85.7                               | 7.89                               |\n| DEA-SQL                            | GPT-4                              | 88.7                               | 89.5                               | 85.6                               | 70.5                               | 85.6                               | 8.69                               |\n| Self-augmented In-Context Learning | Self-augmented In-Context Learning | Self-augmented In-Context Learning | Self-augmented In-Context Learning | Self-augmented In-Context Learning | Self-augmented In-Context Learning | Self-augmented In-Context Learning | Self-augmented In-Context Learning |\n| SAFE-SQL                           | GPT-4                              | 93.2                               | 88.9                               | 85.8                               | 74.7                               | 86.8                               | 21.41                              |\n| SAFE-SQL                           | GPT-4o                             | 93.4                               | 89.3                               | 88.4                               | 75.8                               | 87.9                               | 14.92                              |\n| SAFE-SQL                           | GPT-4o-mini                        | 93.6                               | 87.5                               | 86.1                               | 75.2                               | 87.4                               | 15.33                              |\n| SAFE-SQL                           | Llama3.1-70B-Instruct              | 90.4                               | 88.2                               | 86.2                               | 78.2                               | 86.8                               | 23.52                              |\n| SAFE-SQL                           | Llama3.3-70B-Instruct              | 92.0                               | 80.5                               | 81.0                               | 62.9                               | 80.5                               | 22.46                              |\n| SAFE-SQL                           | Qwen2.5-72B                        | 87.6                               | 74.5                               | 77.0                               | 52.4                               | 74.5                               | 28.51                              |\n| SAFE-SQL                           | Gemma3-12B                         | 92.4                               | 90.7                               | 85.1                               | 78.8                               | 88.2                               | 13.28                              |\n| SAFE-SQL                           | Gemma3-27B                         | 93.6                               | 89.8                               | 87.4                               | 78.8                               | 88.5                               | 14.26                              |  \n(EM), on the other hand, assesses whether the predicted SQL query exactly matches the ground truth query in its structure and syntax. By combining these two metrics, we ensure a comprehensive evaluation of both the correctness and execution reliability of the generated SQL queries."
  },
  {
    "chunk_id": 14,
    "question": [
      "SAFE-SQL shows superior performance in which SQL difficulty categories compared to other methods?",
      "What factors contribute to the improved performance of SAFE-SQL in generating complex SQL queries?",
      "How does SAFE-SQL's execution accuracy on the Bird dataset compare to that of the Syn-SQL method?"
    ],
    "content": "## 4.4 Performance Comparison  \nSpider Dataset We analyze the performance of SAFE-SQL across different SQL difficulty levels and compare it with zero-shot, few-shot prompting methods, and supervised fine-tuning approaches. The results, presented in Table 1, demonstrate that SAFE-SQL achieves overall superior performance, with particularly strong improvements in hard and extra hard categories. Few-shot methods exhibit higher accuracy in Easy and Medium categories, which can be attributed to skeleton-masked selection which retrieves answers directly from the training set, leading to an inflated performance in simpler queries. SAFE-SQL excels in hard and extra hard categories, achieving significantly higher EX. This improvement is notably influenced by the inclusion of reasoning paths, which provide explicit guidance in SQL generation and enhance the model's ability to construct complex queries, as well as the filtering of misleading examples, which reduces potential confusion and prevents error propagation. These multiple factors play a crucial role in enabling the model to generate more accurate and structurally sound SQL queries, especially  \nTable 2: Execution accuracy on Bird dataset.  \n| Method                             | Model                              | Execution Accuracy                 |\n|------------------------------------|------------------------------------|------------------------------------|\n| Supervised Fine-Tuning (SFT)       | Supervised Fine-Tuning (SFT)       | Supervised Fine-Tuning (SFT)       |\n| Syn-SQL SQL-Palm                   | Sense13B Palm                      | 63.4 53.6                          |\n| Zero-shot Methods                  | Zero-shot Methods                  | Zero-shot Methods                  |\n| Baseline                           | GPT-4                              | 49.2                               |\n| Baseline                           | GPT-4o                             | 51.8                               |\n| Baseline                           | GPT-4o-mini                        | 51.2                               |\n| C3-SQL                             | GPT-4                              | 53.8                               |\n| Few-shot Methods                   | Few-shot Methods                   | Few-shot Methods                   |\n| Din-SQL                            | GPT-4                              | 55.9                               |\n| Dail-SQL                           | GPT-4                              | 55.4                               |\n| ACT-SQL                            | GPT-4                              | 52.8                               |\n| PTD-SQL                            | GPT-4                              | 57.0                               |\n| DEA-SQL                            | GPT-4                              | 52.4                               |\n| Self-augmented In-Context Learning | Self-augmented In-Context Learning | Self-augmented In-Context Learning |\n| SAFE-SQL                           | GPT-4                              | 58.9                               |\n| SAFE-SQL                           | GPT-4o                             | 63.5                               |\n| SAFE-SQL                           | GPT-4o-mini                        | 62.1                               |\n| SAFE-SQL                           | Llama3.1-70B-Instruct              | 60.9                               |\n| SAFE-SQL                           | Llama3.3-70B-Instruct              | 61.2                               |\n| SAFE-SQL                           | Qwen2.5-72B                        | 56.2                               |\n| SAFE-SQL                           | Gemma3-12B                         | 60.8                               |\n| SAFE-SQL                           | Gemma3-27B                         | 61.5                               |  \nTable 3: Ablation study results for SAFE-SQL, where removing each component leads to a performance drop.  \n| Models                  | EX           | EM          |\n|-------------------------|--------------|-------------|\n| SAFE-SQL - GPT-4o       | 87.9         | 78.3        |\n| w/o Reasoning path      | 84.4 (-3.5)  | 73.6(-4.7)  |\n| w/o Relevance filtering | 82.1 (-5.8)  | 68.5(-9.7)  |\n| w/o Schema linking      | 80.4 (-7.5)  | 65.1(-13.2) |\n| w/o Similar examples    | 77.1 (-10.8) | 61.9(-16.4) |  \nin challenging scenarios where other approaches struggle. Notably, SAFE-SQL using open-source models such as Gemma3-27B outperforms highcost methods based on GPT-4, highlighting its costeffectiveness and strong capability.  \nBird Dataset We also conduct experiments on the Bird Dev dataset in addition to the Spider dataset. Similar to Spider, SAFE-SQL consistently outperforms zero-shot and few-shot methods, achieving 63.5% execution accuracy with GPT-4o, which is even higher than Syn-SQL (63.4%), a supervised fine-tuning approach. This highlights the effectiveness of our SAFE-SQL using a selfaugmented in-context learning method."
  },
  {
    "chunk_id": 15,
    "question": [
      "Ablation study에서 어떤 네 가지 주요 모듈이 제거되었는가?",
      "Reasoning Path를 제거했을 때 EX 점수에 어떤 변화가 있었는가?",
      "각 구성 요소가 SQL 생성에서 최적의 성능을 달성하는 데 왜 중요한가?"
    ],
    "content": "## 4.5 Ablation Study  \nTo assess the contribution of each key component in our model, we conduct an ablation study by systematically removing four critical modules: Reasoning Path , Relevance Score , Schema Linking , and Similar Examples . We evaluate the resulting impact on performance using EX shown in Table 3. Our findings indicate that each component plays a crucial role in the model's effectiveness. Removing the Reasoning Path leads to a 3.5-point drop in EX, highlighting its importance in guiding the model toward generating accurate SQL queries. The absence of the Relevance Score resulted in a 5.8-point decrease in EX, underscoring its contribution to overall performance. Eliminating Schema Linking causes a 7.5-point drop in EX, which demonstrates its critical role in similar example construction. Overall, each of the four components-Reasoning Path, Relevance Score, Schema Linking, and Similar Examples-is essential for achieving optimal performance in SQL generation, empirically validating our architectural and design choices."
  },
  {
    "chunk_id": 16,
    "question": [
      "SAFE-SQL의 성능이 다른 방법들과 비교했을 때 어떤 특징이 있나요?",
      "임베딩 유사성이 실행 정확도에 미치는 영향은 무엇인가요?",
      "세 가지 측정 요소가 성능에 미치는 영향은 어떻게 나타났나요?"
    ],
    "content": "## 4.6 Analysis  \nInference Time per Query on Spider Dev Set As shown in Table 1, we compare the inference time of SAFE-SQL with other methods. While baseline methods achieve faster inference via simple zero-shot prompts, they show lower accuracy. Few-shot methods are faster than SAFE-SQL but still underperform in execution accuracy. In contrast, SAFE-SQL leverages example augmentation and filtering process, achieving higher performance with a modest increase in inference time. Despite requiring three LLM calls, SAFE-SQL demonstrates strong zero-shot capabilities without relying on a training set, making the trade-off in latency worthwhile.  \nTable 4: Summary of data generation, filtering results, and embedding similarity analysis by score.  \n| Score   |   cos θ |   # of Generated EX | %Filtered EX   |\n|---------|---------|---------------------|----------------|\n| ≥ 0     |   0.581 |               10340 | 0%             |\n| ≥ 2     |   0.625 |               10185 | 1.50% (-155)   |\n| ≥ 4     |   0.744 |                9883 | 4.41% (-457)   |\n| ≥ 6     |   0.762 |                9378 | 9.30% (-962)   |\n| ≥ 8     |   0.765 |                8606 | 16.76% (-1734) |\n| ≥ 10    |   0.769 |                6795 | 34.28% (-3545) |  \nNumber of Generated and Filtered Examples per Score, along with an Embedding Similarity Analysis of the Filtered Examples For each test question in the Spider dev set, 10 examples are generated, resulting in a total of 10,340 examples. The quality of these examples is assessed using a relevance score ranging from 0 to 10. As shown in Table 4, the 65.71% of examples are assigned a score of 10, while the 0.59% of examples are received a score of 0. This trend suggests that the LLM tends to assign high relevance to its own  \nFigure 3: (Left) Correlation between question embedding similarity and average EX, (Right) Average EX across embedding similarity bins  \n<!-- image -->  \ngenerated examples. The similarity is computed using cosine similarity, where higher scores indicate greater semantic alignment between the test questions and the retained examples. As the filtering threshold increases, the embedding similarity also increases, suggesting that higher-relevance examples exhibit stronger semantic consistency with the test questions. However, we also observe that overly strict filtering-selecting only examples with a perfect score of 10-leads to a decline in performance. This drop occurs because an excessively high threshold significantly reduces the number of available examples, limiting the diversity.  \nEffect of Question Embedding Similarity on Execution Accuracy. In Figure 3, the left graph illustrates the correlation between embedding similarity and EX. Each point represents one of the 11 data points obtained by filtering examples based on different threshold scores (0 to 10). The data points follow an upward trend, suggesting that higher similarity tends to result in better EX. The red line indicates the overall correlation, with a coefficient of 0.82, showing a relatively strong positive relationship. Building on this analysis, the right graph provides a more fine-grained view by examining the execution accuracy of individual generated examples based on their embedding similarity with test questions. The x-axis represents the normalized similarity between the test question and the generated question, and the y-axis indicates EX. The results show that EX is lowest in the 0.0-0.1 similarity range, suggesting that examples with very low similarity to test questions tend to be less useful. As similarity increases, EX generally improves, peaking in the 0.7-0.8 range. This suggests that examples with a moderate to high similarity to test questions are more effective in generating  \nFigure 4: Performance of GPT-4o at different relevance score thresholds.  \n<!-- image -->  \nexecutable SQL queries. However, accuracy drops slightly in the 0.8-0.9 range before rising again in the 0.9-1.0 range. This indicates that excessively high similarity can reduce diversity, potentially limiting the model's generalization ability.  \nEffect of Relevance Scoring Thresholds on Performance. To further evaluate the effectiveness of SAFE-SQL, we conduct a detailed case study using varying thresholds for the relevance scoring mechanism as shown in Figure 4. The selfgenerated examples are filtered based on relevance scores, with thresholds ranging from 0 to 10. For each test question, the number of high-scoring examples varied due to the specific content and schema structure (e.g., some test questions have six examples with scores ≥ 8 , while others have three). The selected examples are then used during the final inference stage to generate SQL queries. The ≥ 8 threshold consistently produces the best results, validating the robustness of SAFE-SQL's relevance score filtering. The results demonstrate that selecting high-quality examples plays a critical  \nTable 5: Comparative examples of gold-questions and GPT-4o-generated SQL questions, including detailed reasoning paths and the process of computing relevance scores.  \n<!-- image -->  \n| GOLD Question                                                                                                               | GOLD SQL Query                                                                                                      | Generated Question                                                                                                     | Reasoning Path                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Relevance Score                                                                                             |\n|-----------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\n| Question1: What are the names, countries, and ages for every singer in descending or- der of age?                           | SELECT name, country, age FROM singer ORDER BY age DESC                                                             | What are the names, ages, and countries of all singers from a spe- cific country, sorted by age in descending or- der? | 1. Identify the desired columns: name, age, and country. 2. Specify the table: singer. 3. Sort the results by age in descending order.                                                                                                                                                                                                                                                                                                                                                                    | Semantic similarity = 10 Structural Similarity = 10 Reasoning path = 10 Relevance score = (10+10+10)/3 = 10 |\n| Question2: Return the names and template ids for documents that con- tain the letter w in their description.                | SELECT document_name, template_id FROM Documents WHERE Document_Description LIKE \"%w%\"                              | Retrieve the titles and category IDs of articles whose summaries con- tain the word \"data\".                            | 1. Identify the necessary columns: extract title and category_id from the Articles table. 2. Locate the relevant table: use the Articles table as it contains the required data. 3. Define the filtering condition: apply a WHERE clause to check if the summary column contains the substring \"data\". 4. Use the LIKE operator: employ LIKE '%data%' to search for any instance of \"data\" within the summary. 5. Retrieve the results: return the title and category_id values for all matching records. | Semantic similarity = 7 Structural Similarity = 9 Reasoning path = 8 Relevance score = (7+9+8)/3 = 8        |\n| Question3: What is the number of car models that are produced by each maker and what is the id and full name of each maker? | SELECT Count(*), T2.FullName, T2.id FROM MODEL_LIST AS T1 JOIN CAR_MAKERS AS T2 ON T1.Maker = T2.id GROUP BY T2.id; | List all employees who work in the IT depart- ment along with their employee ID and hire date.                         | 1. Identify required details: employee ID and hire date. 2. Filter condition: find employees who work in IT. 3. Retrieve data: select only emp_id and hire_date.                                                                                                                                                                                                                                                                                                                                          | Semantic similarity = 6 Structural Similarity = 3 Reasoning path = 2 Relevance score = (6+3+2)/3 = 3.67     |  \nTable 6: Execution accuracy across difficulty levels under different weights: semantic similarity ( α ), structural similarity ( β ), and reasoning path quality ( γ ).  \n|    α |    β |    γ |   Easy |   Medium |   Hard |   Extra |   EX |\n|------|------|------|--------|----------|--------|---------|------|\n| 0.33 | 0.33 | 0.33 |   93.4 |     89.3 |   88.4 |    75.8 | 87.9 |\n| 1    | 0    | 0    |   90.7 |     84.2 |   82.3 |    68.3 | 82.8 |\n| 0    | 1    | 0    |   89.8 |     85.6 |   81.2 |    69.2 | 83.1 |\n| 0    | 0    | 1    |   89.2 |     85.1 |   84.3 |    71.7 | 83.7 |\n| 0.5  | 0.5  | 0    |   91.2 |     87.3 |   82.5 |    69.4 | 84.4 |\n| 0.5  | 0    | 0.5  |   92.5 |     87.9 |   83.5 |    70.3 | 85.3 |\n| 0    | 0.5  | 0.5  |   92.7 |     86.8 |   88.5 |    72.4 | 86.1 |  \nrole in guiding LLMs to generate accurate SQL queries, regardless of the underlying model.  \nEffect of Three Measuring Components on Performance. To assess the impact of the three measuring components-semantic similarity ( α ), structural similarity ( β ), and reasoning path quality ( γ )-on EX, we conduct experiments by varying their respective weightings. The results, presented in Table 6, highlight distinct performance trends across different difficulty levels. Notably, the exclusion of reasoning path quality leads to a drop in EX, particularly in the Hard and Extra Hard. This suggests that a well-structured reasoning path is crucial for handling complex queries, as it provides essential logical steps that bridge the gap between natural language understanding and SQL formulation. Conversely, semantic similarity and structural similarity have a greater influence on the Easy and Medium levels. This is because these queries tend to be relatively straightforward, mean- ing that having structurally similar SQL questions in the example set often provides sufficient guidance for generating correct queries. In these cases, direct pattern matching and schema alignment play a larger role. Overall, the findings demonstrate that a balanced combination of all three components is essential for optimizing performance across different levels of query complexity."
  },
  {
    "chunk_id": 17,
    "question": [
      "첫 번째 예제의 관련성 점수는 얼마이며, 그 점수를 얻은 이유는 무엇인가?",
      "두 번째 예제에서 사용된 용어의 차이는 무엇이며, 이로 인해 어떤 점수 변화가 있었는가?",
      "세 번째 예제가 낮은 관련성 점수를 받은 이유는 무엇이며, 그 결과로 나타난 유사성 점수는 얼마인가?"
    ],
    "content": "## 4.7 Case Study  \nAs shown in Table 5, test questions from the Spider dev set alongside their generated similar examples, evaluated based on semantic similarity, structural similarity, and the reasoning path score, which together determine the relevance score. The first example achieves a perfect relevance score of 10, as the generated question closely aligns with the original in meaning, structure, and reasoning. The SQL formulation remains nearly identical, and the reasoning path explicitly details each step, ensuring full alignment. The second example receives a relevance score of 8, with semantic similarity of 7 due to minor differences in terminology (\"documents\" vs. \"articles\" and \"letter 'w'\" vs. \"word 'data'\"). However, its structural similarity remains high, as the SQL structure is nearly identical. The reasoning path score of 8 reflects a clear explanation of query formulation, though slightly less detailed than the first example. The third example has the lowest relevance score due to significant differences. The generated question shifts focus from counting car models to listing IT employees, resulting in semantic similarity of 6 and structural similarity of 3. These results emphasize the importance of fine-grained example selection due to the varing quality of generated examples."
  },
  {
    "chunk_id": 18,
    "question": [
      "SAFE-SQL은 어떤 목적을 가지고 설계되었나요?",
      "SAFE-SQL에서 성능 향상에 기여하는 두 가지 주요 요소는 무엇인가요?",
      "SAFE-SQL의 성능은 어떤 종류의 시나리오에서 특히 두드러진 개선을 보였나요?"
    ],
    "content": "## 5 Conclusion  \nWe introduce SAFE-SQL, a novel unsupervised framework designed for Text-to-SQL. SAFE-SQL generates and filters high-quality self-augmented examples for in-context learning. Extensive experiments demonstrated that both the fine-grained example generation process and optimal threshold filtering contribute significantly to performance gains. Our method achieves state-of-the-art results, showing notable improvements over ablated versions and excelling particularly in challenging extra hard and unseen scenarios."
  },
  {
    "chunk_id": 19,
    "question": [
      "SAFE-SQL의 성능이 뛰어난 이유는 무엇인가요?",
      "현재의 SAFE-SQL 프레임워크가 직면한 제한 사항 중 하나는 무엇인가요?",
      "모델이 다루기 어려운 쿼리 유형은 어떤 것들이 있나요?"
    ],
    "content": "## Limitations  \nWhile SAFE-SQL demonstrates strong performance in generating accurate and semantically valid SQL queries, there are a few limitations that should be addressed in future work. Although the model performs well on the tested datasets, its ability to generalize to highly diverse or domainspecific SQL tasks remains to be fully evaluated. The current framework also relies on large language models like GPT-4o, which may not be easily scalable to low-resource settings or environments with limited computational resources. Handling edge cases and extremely complex queries, which might require deeper schema understanding and more sophisticated reasoning, is another challenge for the model."
  },
  {
    "chunk_id": 20,
    "question": [
      "이 연구에서 SQL 생성 향상은 어떤 방법을 사용하고 있나요?",
      "LLMs가 가질 수 있는 편향을 어떻게 완화하고 있나요?",
      "이 연구는 어떤 종류의 데이터 세트를 사용하며, 그 이유는 무엇인가요?"
    ],
    "content": "## Ethics Statement  \nWhile our approach enhances SQL generation without additional fine-tuning, it relies on LLMs, which may inherit biases from training data. We mitigate potential biases and inaccuracies through structured filtering and relevance scoring. Our study uses publicly available datasets, ensuring compliance with data privacy standards. We encourage responsible use of our method, particularly in applications requiring high accuracy and fairness."
  },
  {
    "chunk_id": 21,
    "question": [
      "이 연구는 어떤 기관의 지원을 받았나요?",
      "지원금은 어떤 프로그램이나 프로젝트에 사용되었나요?",
      "어떤 정부 부처가 이 연구에 자금을 제공했나요?"
    ],
    "content": "## Acknowledgement  \nThis work was supported by the Institute of Information &amp; Communications Technology Planning  \n&amp;Evaluation (IITP) grant funded by the Korea government (MSIT) [RS-2021-II211341, Artificial Intelligence Graduate School Program (Chung-Ang University)] and a project for Collabo R&amp;D between Industry, University, and Research Institute funded by Korea Ministry of SMEs and Startups in 2025.(RS-2025-02323112)."
  },
  {
    "chunk_id": 22,
    "question": [
      "이 문단에 언급된 논문들은 어떤 주제를 다루고 있나요?",
      "다양한 저자들이 연관된 연구들은 어떤 방식으로 텍스트-투-SQL 문제를 해결하려고 하고 있나요?",
      "이 문단에서 언급된 연구 중, 자연어 인터페이스와 관련된 연구는 무엇인가요?"
    ],
    "content": "## References  \nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 .  \nZhenbiao Cao, Yuanlei Zheng, Zhihao Fan, Xiaojin Zhang, Wei Chen, and Xiang Bai. 2024. Rslsql: Robust schema linking in text-to-sql generation. Preprint , arXiv:2411.00073.  \nXuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, lu Chen, Jinshu Lin, and Dongfang Lou. 2023. C3: Zero-shot text-to-sql with chatgpt. Preprint , arXiv:2307.07306.  \nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 .  \nFarida El Boujddaini, Ahmed Laguidi, and Youssef Mejdoub. 2024. A survey on text-to-sql parsing: From rule-based foundations to large language models. In International Conference on Connected Objects and Artificial Intelligence , pages 266-272. Springer.  \nYujian Gan, Xinyun Chen, and Matthew Purver. 2021. Exploring underexplored limitations of cross-domain text-to-sql generalization. Preprint , arXiv:2109.05157.  \nDawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. 2023. Text-to-sql empowered by large language models: A benchmark evaluation. Preprint , arXiv:2308.15363.  \nZijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, and Xiao Huang. 2024. Next-generation database interfaces: A survey of llmbased text-to-sql. Preprint , arXiv:2406.08426.  \nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 .  \nFei Li and Hosagrahar V Jagadish. 2014. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment , 8(1):73-84.  \n- Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. 2023. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Preprint , arXiv:2305.03111.\n- Ruilin Luo, Liyuan Wang, Binghuai Lin, Zicheng Lin, and Yujiu Yang. 2024. Ptd-sql: Partitioning and targeted drilling with llms in text-to-sql. Preprint , arXiv:2409.14082.\n- Ali Mohammadjafari, Anthony S. Maida, and Raju Gottumukkala. 2025. From natural language to sql: Review of llm-based text-to-sql systems. Preprint , arXiv:2410.01066.\n- Mohammadreza Pourreza and Davood Rafiei. 2023. Din-sql: Decomposed in-context learning of text-tosql with self-correction. Preprint , arXiv:2304.11015.\n- Mohammadreza Pourreza, Ruoxi Sun, Hailong Li, Lesly Miculicich, Tomas Pfister, and Sercan O. Arik. 2024. Sql-gen: Bridging the dialect gap for text-to-sql via synthetic data and model merging. Preprint , arXiv:2408.12733.\n- Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. Picard: Parsing incrementally for constrained auto-regressive decoding from language models. Preprint , arXiv:2109.05093.\n- KaShun Shum, Shizhe Diao, and Tong Zhang. 2023. Automatic prompt augmentation and selection with chain-of-thought from labeled data. arXiv preprint arXiv:2302.12822 .\n- Ruoxi Sun, Sercan Ö. Arik, Alex Muzio, Lesly Miculicich, Satya Gundabathula, Pengcheng Yin, Hanjun Dai, Hootan Nakhost, Rajarishi Sinha, Zifeng Wang, and Tomas Pfister. 2024. Sql-palm: Improved large language model adaptation for text-to-sql (extended). Preprint , arXiv:2306.00739.\n- GemmaTeam, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. 2025. Gemma3technical report. arXiv preprint arXiv:2503.19786 .\n- Rishit Toteja, Arindam Sarkar, and Prakash Mandayam Comar. 2025. In-context reinforcement learning with retrieval-augmented generation for text-to-sql. In Proceedings of the 31st International Conference on Computational Linguistics , pages 10390-10397.\n- Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2021. Rat-sql: Relation-aware schema encoding and linking for textto-sql parsers. Preprint , arXiv:1911.04942.\n- Xubin Wang, Jianfei Wu, Yichen Yuan, Mingzhe Li, Deyu Cai, and Weijia Jia. 2024. Demonstration selection for in-context learning via reinforcement learning. Preprint , arXiv:2412.03966.\n- Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. Preprint , arXiv:2212.10560.\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint , arXiv:2201.11903.\n- Niklas Wretblad, Fredrik Gordh Riseby, Rahul Biswas, Amin Ahmadi, and Oskar Holmström. 2024. Understanding the effects of noise in text-to-sql: An examination of the bird-bench benchmark. Preprint , arXiv:2402.12243.\n- An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024a. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115 .\n- Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, and Chang Zhou. 2024b. Synthesizing textto-sql data from weak and strong llms. Preprint , arXiv:2408.03256.\n- Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3911-3921, Brussels, Belgium. Association for Computational Linguistics.\n- Hanchong Zhang, Ruisheng Cao, Lu Chen, Hongshen Xu, and Kai Yu. 2023. Act-sql: In-context learning for text-to-sql with automatically-generated chain-ofthought. Preprint , arXiv:2310.17342.\n- Yiyun Zhao, Jiarong Jiang, Yiqun Hu, Wuwei Lan, Henry Zhu, Anuj Chauhan, Alexander Li, Lin Pan, Jun Wang, Chung-Wei Hang, Sheng Zhang, Marvin Dong, Joe Lilien, Patrick Ng, Zhiguo Wang, Vittorio Castelli, and Bing Xiang. 2022. Importance of synthesizing high-quality data for text-to-sql parsing. Preprint , arXiv:2212.08785."
  },
  {
    "chunk_id": 23,
    "question": [
      "부록의 주제는 무엇인가요?",
      "부록은 문서에서 어떤 역할을 하나요?",
      "부록의 내용은 어떻게 구성될 수 있나요?"
    ],
    "content": "## A Appendix"
  },
  {
    "chunk_id": 24,
    "question": [
      "SAFE-SQL의 목적은 무엇인가요?",
      "B 프롬프트의 역할은 무엇인가요?",
      "이 문단에서 SAFE-SQL에 대해 설명하는 핵심 아이디어는 무엇인가요?"
    ],
    "content": "## B Prompts for SAFE-SQL"
  },
  {
    "chunk_id": 25,
    "question": [
      "What is the main task described in the prompt for example generation?",
      "How many similar questions, SQL queries, and reasoning paths are to be generated?",
      "What are the key aspects to focus on for ensuring high-quality examples?"
    ],
    "content": "## B.1 Prompt for example generation.  \nFor example generation, we use zero shot prompt as shown in the figure 7.  \nYou are a powerful text-to-SQL reasoner. Your task is to generate ten similar questions, ten SQL queries, and ten reasoning paths for how the SQL queries are derived. To ensure high-quality examples, focus on the following three key aspects:"
  },
  {
    "chunk_id": 26,
    "question": [
      "생성된 질문이 테스트 질문과 동일한 의미를 가져야 하는 이유는 무엇인가?",
      "질문을 변형할 때 어떤 요소를 고려해야 하는가?",
      "모호함이나 추가 제약을 피해야 하는 이유는 무엇인가?"
    ],
    "content": "## Semantic Similarity  \nEnsure that all generated questions have the same underlying meaning as the test question. Variations in wording, synonyms, and phrasing are allowed as long as they preserve the intended query objective. Avoid introducing ambiguity or additional constraints that alter the intent."
  },
  {
    "chunk_id": 27,
    "question": [
      "주요 용어의 예시는 무엇인가요?",
      "구조적 유사성에서 기능적 역할과 관계는 어떻게 유지되어야 하나요?",
      "용어가 다를 경우에도 어떤 요소가 동일해야 한다고 언급되었나요?"
    ],
    "content": "## Structural Similarity  \nWhile key terms (such as table names, column names, and numerical values) may vary, their functional roles and relationships should remain intact."
  },
  {
    "chunk_id": 28,
    "question": [
      "SQL 쿼리를 구성하기 위해 필요한 논리적 추론의 일관성은 어떤 요소에 의해 유지되나요?",
      "문단에서 언급된 SQL 작업의 적용 방식은 어떠한 요소들을 포함하나요?",
      "예시 생성을 위한 제로샷 프롬프트는 무엇을 기반으로 하고 있나요?"
    ],
    "content": "## Reasoning Path  \nThe logical reasoning required to construct the SQL query should remain consistent across examples. Clearly outline each step, including how key conditions are identified and mapped to SQL operations.Maintain coherence in how joins, aggregations, filters, and sorting operations are applied. Do not explain me about the result and just give me ten examples.  \n```\n## Schema linking: schema_linking[i] ## Tables: test_table[i] ## Foreign keys: test_foreign_keys[i] ## Question: test_question[i] ## Similar Question: ## SQL query: ## Reasoning Path:\n```  \nTable 7: The zero-shot prompt used for example generation"
  },
  {
    "chunk_id": 29,
    "question": [
      "예제 생성을 위해 어떤 프롬프트를 사용하나요?",
      "그림 8은 무엇을 보여주고 있나요?",
      "제로 샷 프롬프트의 목적은 무엇인가요?"
    ],
    "content": "## B.2 Prompt for filtering examples.  \nFor example generation, we use zero shot prompt as shown in figure 8."
  },
  {
    "chunk_id": 30,
    "question": [
      "최종 추론을 위해 어떤 종류의 프롬프트가 사용되나요?",
      "문단에서 제시된 텍스트-투-SQL 추론기의 역할은 무엇인가요?",
      "예제의 관련성 점수를 계산할 때 고려해야 할 기준은 무엇인가요?"
    ],
    "content": "## B.3 Prompt for final inference.  \nFor final inference, we use zero shot prompt as shown in figure 9.  \nYou are a powerful text-to-SQL reasoner. Given a test question and a set of examples, compute the relevance score for each example based on the following criteria. Do not explain me about the answer, just give me scores."
  },
  {
    "chunk_id": 31,
    "question": [
      "문장에서 점수를 매기는 기준은 무엇인가요?",
      "유사한 의미를 가진 질문들은 어떤 경우에 높은 점수를 받나요?",
      "질문들이 서로 다른 데이터베이스 작업에 초점을 맞출 경우 어떻게 점수가 매겨지나요?"
    ],
    "content": "## Semantic Similarity  \nCompare the overall meaning of the test question and the example question. Higher scores should be assigned if the two questions have the same intent, even if they are phrased differently. Consider synonyms, paraphrasing, and minor wording variations that do not alter the fundamental meaning. Assign lower scores if the test and example questions focus on different database operations (e.g., aggregation vs. filtering) or require fundamentally different types of information.(up to 10 points).  \n- 10: Almost identical meaning and intent.\n- 7-9: Minor paraphrasing but highly relevant.\n- 4-6: Some overlap but different focus.\n- 1-3: Mostly unrelated meaning.\n- 0: Completely different intent."
  },
  {
    "chunk_id": 32,
    "question": [
      "구조적 유사성을 평가할 때 어떤 요소들이 분석되어야 하는가?",
      "점수 시스템에서 10점은 어떤 조건을 의미하는가?",
      "구조적 관계가 약할 때 점수는 어떻게 평가되는가?"
    ],
    "content": "## Structural Similarity  \nEvaluate the structural alignment between the test question and the example question by analyzing how key elements (such as entities, attributes, and numerical values) are connected. Even if individual nouns, verbs, or numbers differ, the overall relational structure should be considered. Focus on whether the dependencies between key components (e.g., how entities relate to each other in the database) remain consistent.(up to 10 points).  \n- 10: Nearly identical structural relationships and dependencies. 7-9: Mostly similar structure, with minor differences in entity connections.\n- 4-6: Some overlap, but noticeable differences in how key components interact.\n- 1-3: Few shared structural relationships, making alignment weak.\n- 0: No meaningful structural similarities."
  },
  {
    "chunk_id": 33,
    "question": [
      "이 문단에서 SQL 쿼리를 도출하기 위한 논리적 단계를 평가할 때 고려해야 할 주요 요소는 무엇인가?",
      "예시 질문과 테스트 질문 간의 유사성을 평가하기 위해 어떤 기준이 사용되는가?",
      "점수 체계는 어떻게 구성되어 있으며, 각 점수는 어떤 의미를 갖는가?"
    ],
    "content": "## Reasoning Path  \nEvaluate whether the logical steps needed to answer the example question align with those required for the test question. Consider whether the database operations (e.g., filtering, aggregation, joins, subqueries) are similar.A high score should be given if the example follows the same logical sequence to derive the SQL query.Lower scores should be assigned if the reasoning process differs significantly, even if the questions seem similar at a surface level.(up to 10 points).  \n- 10: Exact reasoning process to get right SQL query.\n- 7-9: Mostly similar but with minor differences.\n- 4-6: Some alignment but different key steps.\n- 1-3: Largely different reasoning.\n- 0: Completely unrelated logic.  \n- ## Question: test\\_question[i]  \n- ## Similar Question:  \nsimilar\\_question[i]  \n- ## Reasoning Path: reasoning\\_path[i]  \n- ## Relevance score:  \nTable 8: The zero-shot prompt used for filtering examples.  \nYou are a powerful text-to-SQL reasoner. Your task is to generate the final SQL query using a set of selected examples that provide guidance on query construction. Utilizing Selected Examples. Do not explain me about the answer, just give me SQL query.  \nA set of chosen examples, each containing: A natural language question similar to the test question A corresponding SQL query A detailed reasoning path explaining how the SQL query was derived These examples are selected based on three key criteria:  \nSemantic Similarity The selected examples closely match the intent of the test question. Variations in wording do not change the meaning.  \nStructural Similarity The database schema elements (tables, columns, joins) used in the examples align with the test question. The SQL syntax and structure are relevant to the expected query.  \nReasoning Path Similarity The logical steps used to construct the SQL query align with the reasoning required for the test question. Key transformations, filtering conditions, and aggregation logic are similar."
  },
  {
    "chunk_id": 34,
    "question": [
      "최종 SQL 쿼리를 생성하기 위해 어떤 절차를 따라야 합니까?",
      "Qwen 2.5 시리즈 모델의 정확도는 난이도에 따라 어떻게 달라집니까?",
      "테이블 9와 테이블 10의 주요 차이점은 무엇인가요?"
    ],
    "content": "## Final SQL Query Construction  \nUsing the selected examples, generate the final SQL query that correctly retrieves the desired result for the given test question. Follow the reasoning patterns observed in the examples. Now, generate the final SQL query for the given test question:  \n##Tables:  \ntest\\_table[i]  \n##Foreign\\_keys: test\\_foreign\\_keys[i]  \n##Question:  \ntext\\_question[i]  \n##Filtered\\_example:  \nfiltered\\_example[i]  \nTable 9: The zero-shot prompt used for Final SQL query inference.  \n|              |   Easy |   Med |   Hard |   Extra |   All |\n|--------------|--------|-------|--------|---------|-------|\n| Qwen 2.5-3B  |   62.4 |  61.2 |   58.6 |    48.8 |  59.1 |\n| Qwen 2.5-7B  |   80   |  78   |   67.2 |    51.8 |  72.3 |\n| Qwen 2.5-14B |   81.2 |  80.3 |   69.5 |    56.4 |  74.7 |  \nTable 10: Execution accuracy performance of different size of models of Qwen series across difficulty levels of spider dev set."
  },
  {
    "chunk_id": 35,
    "question": [
      "모델 크기가 예제 생성에 미치는 영향은 무엇인가?",
      "Qwen2.5 모델의 성능에서 14B 모델이 차지하는 위치는?",
      "더 큰 모델이 SQL 쿼리 생성을 개선하는 이유는 무엇인가?"
    ],
    "content": "## C Impact of model size  \nPerformance based on generated examples across different model size As shown in Table 10, We investigate the impact of model size on example generation with different variants of the Qwen2.5 Models. The results demonstrate that the 14B model achieves the highest overall performance, followed by the 7B and the 3B. This trend is consistent across all difficulty levels, with large model size generating higher-quality examples that lead to more accurate SQL query generation. The performance improvement with increasing model size can be attributed to the enhanced capacity of larger models to capture SQL question patterns and semantic relationships. Moreover, larger models possess more extensive information, allowing them to generate more appropriate questions and construct detailed reasoning paths, which contribute to the overall accuracy of SQL query generation."
  },
  {
    "chunk_id": 36,
    "question": [
      "What does Figure 5 illustrate regarding the spider dev set?",
      "Why might questions in the same category lead to inconsistencies in training set retrieval?",
      "What limitation is highlighted in the context of Text-to-SQL tasks?"
    ],
    "content": "## D Spider dev training set embedding clusters.  \nFigure 5: Embedding of spider dev set training questions.  \n<!-- image -->  \nAlthough questions within the same category share semantic similarities, they may belong to different clusters, leading to inconsistencies when retrieving examples from the training set. This highlights the limitations of training set retrieval in Text-to-SQL tasks."
  }
]